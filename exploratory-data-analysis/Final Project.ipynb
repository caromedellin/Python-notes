{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congress Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>in_office</th>\n",
       "      <th>...</th>\n",
       "      <th>govtrack_id</th>\n",
       "      <th>crp_id</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>congresspedia_url</th>\n",
       "      <th>youtube_url</th>\n",
       "      <th>facebook_id</th>\n",
       "      <th>official_rss</th>\n",
       "      <th>senate_class</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>oc_email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rep</td>\n",
       "      <td>Neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>400001</td>\n",
       "      <td>N00007665</td>\n",
       "      <td>neilabercrombie</td>\n",
       "      <td>http://www.opencongress.org/wiki/Neil_Abercrombie</td>\n",
       "      <td>http://youtube.com/hawaiirep1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1938-06-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rep</td>\n",
       "      <td>Gary</td>\n",
       "      <td>L.</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NY</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>400003</td>\n",
       "      <td>N00001143</td>\n",
       "      <td>repgaryackerman</td>\n",
       "      <td>http://www.opencongress.org/wiki/Gary_Ackerman</td>\n",
       "      <td>http://youtube.com/RepAckerman</td>\n",
       "      <td>RepAcherman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1942-11-19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rep</td>\n",
       "      <td>Robert</td>\n",
       "      <td>B.</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AL</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>400004</td>\n",
       "      <td>N00003028</td>\n",
       "      <td>Robert_Aderholt</td>\n",
       "      <td>http://www.opencongress.org/wiki/Robert_Aderholt</td>\n",
       "      <td>http://youtube.com/RobertAderholt</td>\n",
       "      <td>19787529402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1965-07-22</td>\n",
       "      <td>Rep.Aderholt@opencongress.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sen</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Kahikina</td>\n",
       "      <td>Akaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>300001</td>\n",
       "      <td>N00007653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.opencongress.org/wiki/Daniel_Akaka</td>\n",
       "      <td>http://youtube.com/senatorakaka</td>\n",
       "      <td>danielakaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>1924-09-11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sen</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>A.</td>\n",
       "      <td>Allard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>CO</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>300003</td>\n",
       "      <td>N00009082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.opencongress.org/wiki/Wayne_Allard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>II</td>\n",
       "      <td>1943-12-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  title firstname middlename     lastname name_suffix nickname party state  \\\n",
       "0   Rep      Neil        NaN  Abercrombie         NaN      NaN     D    HI   \n",
       "1   Rep      Gary         L.     Ackerman         NaN      NaN     D    NY   \n",
       "2   Rep    Robert         B.     Aderholt         NaN      NaN     R    AL   \n",
       "3   Sen    Daniel   Kahikina        Akaka         NaN      NaN     D    HI   \n",
       "4   Sen     Wayne         A.       Allard         NaN      NaN     R    CO   \n",
       "\n",
       "      district  in_office              ...               govtrack_id  \\\n",
       "0            1          0              ...                    400001   \n",
       "1            5          0              ...                    400003   \n",
       "2            4          1              ...                    400004   \n",
       "3  Junior Seat          0              ...                    300001   \n",
       "4  Senior Seat          0              ...                    300003   \n",
       "\n",
       "      crp_id       twitter_id  \\\n",
       "0  N00007665  neilabercrombie   \n",
       "1  N00001143  repgaryackerman   \n",
       "2  N00003028  Robert_Aderholt   \n",
       "3  N00007653              NaN   \n",
       "4  N00009082              NaN   \n",
       "\n",
       "                                   congresspedia_url  \\\n",
       "0  http://www.opencongress.org/wiki/Neil_Abercrombie   \n",
       "1     http://www.opencongress.org/wiki/Gary_Ackerman   \n",
       "2   http://www.opencongress.org/wiki/Robert_Aderholt   \n",
       "3      http://www.opencongress.org/wiki/Daniel_Akaka   \n",
       "4      http://www.opencongress.org/wiki/Wayne_Allard   \n",
       "\n",
       "                         youtube_url  facebook_id official_rss  senate_class  \\\n",
       "0      http://youtube.com/hawaiirep1          NaN          NaN           NaN   \n",
       "1     http://youtube.com/RepAckerman  RepAcherman          NaN           NaN   \n",
       "2  http://youtube.com/RobertAderholt  19787529402          NaN           NaN   \n",
       "3    http://youtube.com/senatorakaka  danielakaka          NaN             I   \n",
       "4                                NaN          NaN          NaN            II   \n",
       "\n",
       "    birthdate                       oc_email  \n",
       "0  1938-06-26                            NaN  \n",
       "1  1942-11-19                            NaN  \n",
       "2  1965-07-22  Rep.Aderholt@opencongress.org  \n",
       "3  1924-09-11                            NaN  \n",
       "4  1943-12-02                            NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_words = pd.read_csv(\"../data/legislators.csv\")\n",
    "congress_words = pd.DataFrame(congress_words)\n",
    "congress_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'firstname',\n",
       " 'middlename',\n",
       " 'lastname',\n",
       " 'name_suffix',\n",
       " 'nickname',\n",
       " 'party',\n",
       " 'state',\n",
       " 'district',\n",
       " 'in_office',\n",
       " 'gender',\n",
       " 'phone',\n",
       " 'fax',\n",
       " 'website',\n",
       " 'webform',\n",
       " 'congress_office',\n",
       " 'bioguide_id',\n",
       " 'votesmart_id',\n",
       " 'fec_id',\n",
       " 'govtrack_id',\n",
       " 'crp_id',\n",
       " 'twitter_id',\n",
       " 'congresspedia_url',\n",
       " 'youtube_url',\n",
       " 'facebook_id',\n",
       " 'official_rss',\n",
       " 'senate_class',\n",
       " 'birthdate',\n",
       " 'oc_email']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_words.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API call\n",
    "I used the bioGuide id to retrive the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A000014', 'A000022', 'A000055', 'A000069', 'A000109', 'A000210', 'A000357', 'A000358', 'A000360', 'A000361']\n"
     ]
    }
   ],
   "source": [
    "l_bioGuides = congress_words.bioguide_id.tolist()\n",
    "print l_bioGuides[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API call method it can be tweeked to scrape other data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import Request, urlopen\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "def requestWords( id ):\n",
    "\n",
    "    id = str(id)\n",
    "    url = \"http://capitolwords.org/api/1/phrases.json?entity_type=legislator&entity_value=\"+id+\"&apikey=0bf8e7eb6ce146f48217bfee767c998d\"\n",
    "    request=Request(url)\n",
    "    response = urlopen(request)\n",
    "    contents = response.read()\n",
    "    len(contents)\n",
    "    if len(contents) > 2:\n",
    "        data = json.loads(contents)\n",
    "        words = json_normalize(data)\n",
    "        list_of_words = words.ngram.tolist()\n",
    "        string_of_words =\"|\".join(list_of_words)\n",
    "        return string_of_words\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "congress_words['favorite_words'] = congress_words.apply(lambda row: requestWords(row['bioguide_id']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     hawaiian|hawaii|hawaiians|hawaii's|kalaupapa|e...\n",
       "1     queens|rabbi|jewish|bayside|flushing|nassau|br...\n",
       "2     aderholt|requesting|irons|huntsville|alabama|r...\n",
       "3     hawaii's|hawaii|hawaiians|hawaiian|dsh|va|fas|...\n",
       "4     colorado|flats|missile|rocky|colorado's|denver...\n",
       "5     camden|gloucester|cyprus|rutgers|opic|jersey|p...\n",
       "6     mercury|maine|prescription|pharmaceutical|drug...\n",
       "7     freddie|morgenthau|fannie|you've|pilgrims|mayf...\n",
       "8     tennesseans|carbon-free|tennessee|electricity|...\n",
       "9     rodney|baton|rouge|lsu|louisiana|la|ruston|req...\n",
       "10    murphy|drill|pittsburgh|altmire|sbir|oil|anwr|...\n",
       "11    upstate|herkimer|utica|tay-sachs|suny|cybersec...\n",
       "12    nj|2009|objection|army|minutes|recognized|6|se...\n",
       "13    fairborn|ohio's|xenia|gire|wright-patterson|wa...\n",
       "14    endeavour|skyler|meanings|shuttle|eagle|scout|...\n",
       "15    foia|8015|aumf|government-set|davis-bacon|1034...\n",
       "16    hampshire|guantanamo|timberland|gitmo|detainin...\n",
       "17                                                  NaN\n",
       "18    alean|brock|gantt|anderton|calvin's|sickle|cal...\n",
       "19                                                  NaN\n",
       "Name: favorite_words, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_words.favorite_words.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean responses that where null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "congress_words = congress_words[congress_words.favorite_words.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of legislators with word record: 763\n"
     ]
    }
   ],
   "source": [
    "print \"Number of legislators with word record:\", len(congress_words.favorite_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build words matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   $0  $1  $1.50  $10  $100  $1000  $100000  $1000000  $10638425746293  $107  \\\n",
      "0   0   0      0    0     0      0        0         0                0     0   \n",
      "1   0   0      0    0     0      0        0         0                0     0   \n",
      "2   0   0      0    0     0      0        0         0                0     0   \n",
      "\n",
      "   ...   ziegler  zimbabwe  zimmer  zinc  zion  zoberman  zone  zones  zoo  \\\n",
      "0  ...         0         0       0     0     0         0     0      0    0   \n",
      "1  ...         0         0       0     0     0         0     0      0    0   \n",
      "2  ...         0         0       0     0     0         0     0      0    0   \n",
      "\n",
      "   zuni  \n",
      "0     0  \n",
      "1     0  \n",
      "2     0  \n",
      "\n",
      "[3 rows x 14420 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'$0', u'$1', u'$1.50', u'$10', u'$100', u'$1000', u'$100000',\n",
       "       u'$1000000', u'$10638425746293', u'$107', u'$12', u'$120', u'$12000',\n",
       "       u'$13', u'$1300', u'$139', u'$14', u'$1400', u'$15', u'$1500',\n",
       "       u'$150000', u'$1500000', u'$159', u'$1600', u'$17', u'$170', u'$18',\n",
       "       u'$186', u'$19', u'$191', u'$2', u'$2.33', u'$200', u'$2000',\n",
       "       u'$200000', u'$2000000', u'$21', u'$23', u'$23000', u'$236', u'$240',\n",
       "       u'$25', u'$250', u'$250000', u'$2500000', u'$270', u'$27000', u'$290',\n",
       "       u'$29000', u'$3', u'$300', u'$3000', u'$30000', u'$300000', u'$30500',\n",
       "       u'$310', u'$319', u'$3300', u'$35', u'$350', u'$35000', u'$350000',\n",
       "       u'$38', u'$4', u'$4.50', u'$400', u'$400000', u'$45', u'$46', u'$464',\n",
       "       u'$5', u'$50', u'$500', u'$5000', u'$50000', u'$500000', u'$5100000',\n",
       "       u'$58', u'$58000', u'$6', u'$60', u'$600', u'$600000', u'$6000000',\n",
       "       u'$61', u'$683', u'$700', u'$713', u'$730', u'$750', u'$750000',\n",
       "       u'$760', u'$787', u'$8', u'$80', u'$800', u'$81', u'$87', u'$90',\n",
       "       u'$900'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favorite_words = congress_words.favorite_words.str.get_dummies(sep = \"|\")\n",
    "print favorite_words.head(3)\n",
    "favorite_words.columns[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(763, 14420)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favorite_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: a lot of numbers!!!!\n",
    "Live API so I can't hard code where the word columns start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'944', u'95', u'952', u'953', u'96', u'964', u'97', u'98', u'9800',\n",
       "       u'9896', u'990', u'991', u'9946', u'999', u'9:30', u'?', u'a', u'a&m',\n",
       "       u'a-plus', u'a.', u'a.d.', u'a.m.', u'a.m.e.', u'aaa', u'aacute',\n",
       "       u'aahsa', u'aamodt', u'aana', u'aapg', u'aapi', u'aapis', u'aaron',\n",
       "       u'aarp', u'abandon', u'abandoned', u'abaya', u'abbas', u'abbas's',\n",
       "       u'abbeville', u'abby'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favorite_words.columns[760:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the words in it [u'abducted', u'abduction', u'abductions', u'abdullah', u'abel', u'abercrombie', u'aberdeen', u'abernathy', u'abilene', u'abilities', u'ability', u'abilityone', u'abington', u'able', u'able-bodied', u'abm', u'aboard', u'abolitionist', u'abortion', u'abortionist', u'abortions', u'about', u'abraham', u'abscissa', u'absence', u'absent', u'absentee', u'absolutely', u'abstinence', u'absurd', u'abu', u'abu-jamal', u'abundance', u'abundant', u'abuse', u'abused', u'abuses', u'abusive', u'aca', u'acacia', u'academic', u'academics', u'academies', u'academy', u'acadiana', u'accept', u'acceptable', u'acceptance', u'access', u'accessibility', u'accessible', u'accessing', u'accidental', u'accidents', u'accolades', u'accommodate', u'accompanying', u'accomplishments', u'accordance', u'accordingly', u'accords', u'account', u'accountability', u'accountable', u'accounting', u'accounts', u'accreditation', u'accreditors', u'accrual', u'accrued', u'accumulate', u'accumulated', u'accuracy', u'accurate', u'accused', u'accutane', u'acequia', u'achieve', u'achievement', u'achievements', u'achieving', u'aci', u'acid', u'acidification', u'acin', u'ackerman', u'acknowledge', u'acme', u'acorn', u'acosta', u'acquire', u'acquisition', u'acre', u'acre-feet', u'acres', u'across', u'across-the-board', u'act', u'acting', u'action']\n"
     ]
    }
   ],
   "source": [
    "word_list = favorite_words.columns.tolist()\n",
    "print \"Some of the words in it\", word_list[800:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "def word_finder(list,start):\n",
    "    for index, element in enumerate(list, start):\n",
    "        if element[0]!=\"a\":\n",
    "            pass\n",
    "        else:\n",
    "            first = index\n",
    "            break\n",
    "    return first\n",
    "x = word_list\n",
    "print word_finder(x,0)\n",
    "\n",
    "def wordlist(dataFrame):\n",
    "    list = dataFrame.columns.tolist()\n",
    "    for index, element in enumerate(list):\n",
    "        if element[0]!=\"a\":\n",
    "            pass\n",
    "        else:\n",
    "            first = index\n",
    "            break\n",
    "    return list[first:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a&m\n"
     ]
    }
   ],
   "source": [
    "print word_list[776]\n",
    "#del favorite_words['a']\n",
    "word_list = favorite_words.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give words a count that is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus\n",
    "I used the whole must repited words to have more of a global tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14419"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = favorite_words.columns.tolist()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'economy', u'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'africans',\n",
       " u'after',\n",
       " u'aftermath',\n",
       " u'afternoon',\n",
       " u'afterschool',\n",
       " u'afterward',\n",
       " u'ag',\n",
       " u'again',\n",
       " u'against',\n",
       " u'agana']"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print analyze(\"economy a this\")\n",
    "vectorizer.get_feature_names()[910:920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4357"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document') #not seen in the training corpus will be completely ignored in future calls to the transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13670"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrelated = vectorizer.transform(['Something completely unrelated']).toarray()\n",
    "len(unrelated[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for later analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'lalalalalalalala',\n",
       " u'some',\n",
       " u'piece',\n",
       " u'of',\n",
       " u'text',\n",
       " u'want',\n",
       " u'to',\n",
       " u'classify',\n",
       " u'for',\n",
       " u'being',\n",
       " u'as',\n",
       " u'rejecting',\n",
       " u'discrimination']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_text_vector(string):\n",
    "    array = analyze(string)\n",
    "    #array = vectorizer.transform([string]).toarray()\n",
    "    return array\n",
    "new_text_vector(\"lalalalalalalala Some piece of text I want to classify for being as rejecting discrimination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.25227343,  4.93051759,  6.94542061, ...,  6.94542061,\n",
       "        6.5399555 ,  6.94542061])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = transformer.fit_transform(favorite_words)\n",
    "tfidf_array = tfidf.toarray()\n",
    "tfidf_array.shape\n",
    "tfidf_array[20].max()\n",
    "transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-464-a324c0612fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manalyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iraq this a unanana\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iraq this a unanana\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \"\"\"\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    765\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"iraq this a unanana\")\n",
    "v = CountVectorizer().fit(\"iraq this a unanana\").vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer.fit_transform(corpus)\n",
    "vec_idf = vectorizer.idf_\n",
    "print len(vec_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "            ...\n",
      "            886, 887, 888, 889, 890, 891, 892, 893, 894, 896],\n",
      "           dtype='int64', length=763)\n",
      "Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "            ...\n",
      "            886, 887, 888, 889, 890, 891, 892, 893, 894, 896],\n",
      "           dtype='int64', length=763)\n"
     ]
    }
   ],
   "source": [
    "words_weight = pd.DataFrame(tfidf_array, index=congress_words.index , columns=corpus)\n",
    "print congress_words.index\n",
    "print words_weight.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>in_office</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rep</td>\n",
       "      <td>Neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rep</td>\n",
       "      <td>Gary</td>\n",
       "      <td>L.</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NY</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rep</td>\n",
       "      <td>Robert</td>\n",
       "      <td>B.</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AL</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sen</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Kahikina</td>\n",
       "      <td>Akaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sen</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>A.</td>\n",
       "      <td>Allard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>CO</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14449 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  title_x firstname middlename     lastname name_suffix nickname party_x  \\\n",
       "0     Rep      Neil        NaN  Abercrombie         NaN      NaN       D   \n",
       "1     Rep      Gary         L.     Ackerman         NaN      NaN       D   \n",
       "2     Rep    Robert         B.     Aderholt         NaN      NaN       R   \n",
       "3     Sen    Daniel   Kahikina        Akaka         NaN      NaN       D   \n",
       "4     Sen     Wayne         A.       Allard         NaN      NaN       R   \n",
       "\n",
       "  state_x   district_x  in_office  ...  ziegler zimbabwe zimmer zinc zion  \\\n",
       "0      HI            1          0  ...      0.0      0.0    0.0  0.0  0.0   \n",
       "1      NY            5          0  ...      0.0      0.0    0.0  0.0  0.0   \n",
       "2      AL            4          1  ...      0.0      0.0    0.0  0.0  0.0   \n",
       "3      HI  Junior Seat          0  ...      0.0      0.0    0.0  0.0  0.0   \n",
       "4      CO  Senior Seat          0  ...      0.0      0.0    0.0  0.0  0.0   \n",
       "\n",
       "  zoberman zone  zones  zoo  zuni  \n",
       "0      0.0  0.0    0.0  0.0   0.0  \n",
       "1      0.0  0.0    0.0  0.0   0.0  \n",
       "2      0.0  0.0    0.0  0.0   0.0  \n",
       "3      0.0  0.0    0.0  0.0   0.0  \n",
       "4      0.0  0.0    0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 14449 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitol_words = congress_words.merge(words_weight, right_index=True, left_index=True)\n",
    "capitol_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a&amp;m</th>\n",
       "      <th>a-plus</th>\n",
       "      <th>a.</th>\n",
       "      <th>a.d.</th>\n",
       "      <th>a.m.</th>\n",
       "      <th>a.m.e.</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aacute</th>\n",
       "      <th>aahsa</th>\n",
       "      <th>aamodt</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13643 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a&m  a-plus   a.  a.d.      a.m.  a.m.e.  aaa  aacute  aahsa  aamodt  ...   \\\n",
       "0  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "1  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "2  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "3  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "4  0.0     0.0  0.0   0.0  0.096384     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "\n",
       "   ziegler  zimbabwe  zimmer  zinc  zion  zoberman  zone  zones  zoo  zuni  \n",
       "0      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "1      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "2      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "3      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "4      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 13643 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_column_names_capitol = capitol_words.columns.tolist()[word_finder(capitol_words,0):]\n",
    "capitol_words[word_column_names_capitol].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.815679735406123"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitol_words[word_column_names_capitol].sum().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of the words that 95% of the people said\n",
    "Because they don't add much in to analyzing what makes legislators different from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequencies = (capitol_words[word_column_names_capitol]>0).astype(int).sum(axis=0).astype(float)/capitol_words.shape[0]\n",
    "most_frequent_words = word_frequencies[word_frequencies>.95].index\n",
    "most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequencies = (capitol_words[word_column_names_capitol]>0).astype(int).sum(axis=0)\n",
    "word_frequencies.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before getting rid of independents: 763\n",
      "Entries after getting rid of independents: 760\n",
      "Number of independents: 3\n"
     ]
    }
   ],
   "source": [
    "capitol_words.party_x.unique()\n",
    "party_mask = capitol_words.party_x!=\"I\"\n",
    "two_party_words = capitol_words[party_mask]\n",
    "print \"Entries before getting rid of independents:\", capitol_words.shape[0]\n",
    "print \"Entries after getting rid of independents:\", two_party_words.shape[0]\n",
    "print \"Number of independents:\", (capitol_words.shape[0])-(two_party_words.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Party Dummies assigning 1 to Republicans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Gary</td>\n",
       "      <td>L.</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NY</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Robert</td>\n",
       "      <td>B.</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AL</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Kahikina</td>\n",
       "      <td>Akaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>A.</td>\n",
       "      <td>Allard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>CO</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   R title_x firstname middlename     lastname name_suffix nickname party_x  \\\n",
       "0  0     Rep      Neil        NaN  Abercrombie         NaN      NaN       D   \n",
       "1  0     Rep      Gary         L.     Ackerman         NaN      NaN       D   \n",
       "2  1     Rep    Robert         B.     Aderholt         NaN      NaN       R   \n",
       "3  0     Sen    Daniel   Kahikina        Akaka         NaN      NaN       D   \n",
       "4  1     Sen     Wayne         A.       Allard         NaN      NaN       R   \n",
       "\n",
       "  state_x   district_x ...   ziegler zimbabwe zimmer zinc zion zoberman zone  \\\n",
       "0      HI            1 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "1      NY            5 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "2      AL            4 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "3      HI  Junior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "4      CO  Senior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "\n",
       "  zones  zoo zuni  \n",
       "0   0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 14450 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party_dummies = pd.get_dummies(capitol_words.party_x).astype(int)\n",
    "party_dummies = party_dummies[[\"R\"]]\n",
    "party_dummies.head()\n",
    "capitol_words = party_dummies.merge(capitol_words, right_index=True, left_index=True)\n",
    "\n",
    "capitol_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_words = capitol_words[word_column_names_capitol]\n",
    "y_words = capitol_words[\"R\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_words,y_words,test_size=0.4)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "words_tree = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "words_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_tree.feature_importances_\n",
    "features = pd.DataFrame({'feature':word_column_names_capitol, 'importance':words_tree.feature_importances_}).sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10505</th>\n",
       "      <td>requesting</td>\n",
       "      <td>0.504671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>spending</td>\n",
       "      <td>0.307311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>republican</td>\n",
       "      <td>0.142692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.045326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9098</th>\n",
       "      <td>overpayment</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  importance\n",
       "10505   requesting    0.504671\n",
       "11635     spending    0.307311\n",
       "10499   republican    0.142692\n",
       "3865           dog    0.045326\n",
       "9098   overpayment    0.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful methods\n",
    "a mask and groupby for dataframe access, something to delete sum 0 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_mask(df,column,condition,value):\n",
    "    new_data = []\n",
    "    if condition == \"==\":\n",
    "        new_data = df[df[column] == value]\n",
    "    elif condition == \"<=\":\n",
    "        new_data = df[df[column] <= value]\n",
    "    elif condition == \"!=\":\n",
    "        new_data = df[df[column] != value]\n",
    "    elif condition == \">=\":\n",
    "        new_data = df[df[column] >= value]\n",
    "    elif condition == \">\":\n",
    "        new_data = df[df[column] > value]\n",
    "    elif condition == \"<\":\n",
    "        new_data = df[df[column] < value]\n",
    "    else:\n",
    "        print \"arguments needed-column,condition,value-:\"\n",
    "    return new_data \n",
    "def subset(df,column):\n",
    "    dict = {}\n",
    "    subs = df[column].unique()  \n",
    "    for element in subs:\n",
    "         dict[element] = my_mask(df,column,\"==\",element)\n",
    "    print \"New available dictionary of dataframes is:\\n subsets_of \",subs \n",
    "    return dict  \n",
    "def clean_sparse_irrelevant(pd):\n",
    "    cols = pd.columns\n",
    "    deleted=0\n",
    "    for c in cols:\n",
    "        x=pd[c]\n",
    "        if x.dtype==\"float64\":\n",
    "            if x.sum()==0:\n",
    "                del pd[c]\n",
    "                deleted += 1\n",
    "    print \"DELETED:\",deleted\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New available dictionary of dataframes is:\n",
      " subsets_of  ['HI' 'NY' 'AL' 'CO' 'NJ' 'ME' 'MO' 'TN' 'LA' 'PA' 'OH' 'FL' 'MI' 'NH' 'NC'\n",
      " 'MD' 'TX' 'MT' 'CA' 'UT' 'AR' 'DE' 'NM' 'GA' 'OR' 'IA' 'VA' 'KS' 'KY' 'IN'\n",
      " 'WV' 'WA' 'WI' 'NV' 'IL' 'SC' 'GU' 'OK' 'MN' 'WY' 'AK' 'MA' 'ND' 'CT' 'VI'\n",
      " 'MS' 'ID' 'RI' 'AS' 'AZ' 'NE' 'PR' 'SD' 'VT' 'DC' 'MP']\n",
      "New available dictionary of dataframes is:\n",
      " subsets_of  ['D' 'R' 'I']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Gary</td>\n",
       "      <td>L.</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NY</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Kahikina</td>\n",
       "      <td>Akaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Robert</td>\n",
       "      <td>E.</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rob</td>\n",
       "      <td>D</td>\n",
       "      <td>NJ</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>H.</td>\n",
       "      <td>Allen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tom</td>\n",
       "      <td>D</td>\n",
       "      <td>ME</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   R title_x firstname middlename     lastname name_suffix nickname party_x  \\\n",
       "0  0     Rep      Neil        NaN  Abercrombie         NaN      NaN       D   \n",
       "1  0     Rep      Gary         L.     Ackerman         NaN      NaN       D   \n",
       "3  0     Sen    Daniel   Kahikina        Akaka         NaN      NaN       D   \n",
       "5  0     Rep    Robert         E.      Andrews         NaN      Rob       D   \n",
       "6  0     Rep    Thomas         H.        Allen         NaN      Tom       D   \n",
       "\n",
       "  state_x   district_x ...   ziegler zimbabwe zimmer zinc zion zoberman zone  \\\n",
       "0      HI            1 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "1      NY            5 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "3      HI  Junior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "5      NJ            1 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "6      ME            1 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "\n",
       "  zones  zoo zuni  \n",
       "0   0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  \n",
       "5   0.0  0.0  0.0  \n",
       "6   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 14450 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = subset(capitol_words,\"state_x\")\n",
    "states['AK'].head()\n",
    "parties = subset(capitol_words, \"party_x\")\n",
    "parties['D'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETED: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alaskans</th>\n",
       "      <td>alaskans</td>\n",
       "      <td>0.496549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchorage</th>\n",
       "      <td>anchorage</td>\n",
       "      <td>0.496549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alaska's</th>\n",
       "      <td>alaska's</td>\n",
       "      <td>0.481533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alaskan</th>\n",
       "      <td>alaskan</td>\n",
       "      <td>0.481533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arctic</th>\n",
       "      <td>arctic</td>\n",
       "      <td>0.431613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fishing</th>\n",
       "      <td>fishing</td>\n",
       "      <td>0.382376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fairbanks</th>\n",
       "      <td>fairbanks</td>\n",
       "      <td>0.379415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ak</th>\n",
       "      <td>ak</td>\n",
       "      <td>0.379415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alaska</th>\n",
       "      <td>alaska</td>\n",
       "      <td>0.378358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tongass</th>\n",
       "      <td>tongass</td>\n",
       "      <td>0.376191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natives</th>\n",
       "      <td>natives</td>\n",
       "      <td>0.365874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statehood</th>\n",
       "      <td>statehood</td>\n",
       "      <td>0.364736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>fish</td>\n",
       "      <td>0.360768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pipeline</th>\n",
       "      <td>pipeline</td>\n",
       "      <td>0.338675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wildlife</th>\n",
       "      <td>wildlife</td>\n",
       "      <td>0.306912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refuge</th>\n",
       "      <td>refuge</td>\n",
       "      <td>0.289692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timber</th>\n",
       "      <td>timber</td>\n",
       "      <td>0.283745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>murkowski</th>\n",
       "      <td>murkowski</td>\n",
       "      <td>0.272302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>land</td>\n",
       "      <td>0.270684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conveyance</th>\n",
       "      <td>conveyance</td>\n",
       "      <td>0.266318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature     words\n",
       "alaskans      alaskans  0.496549\n",
       "anchorage    anchorage  0.496549\n",
       "alaska's      alaska's  0.481533\n",
       "alaskan        alaskan  0.481533\n",
       "arctic          arctic  0.431613\n",
       "fishing        fishing  0.382376\n",
       "fairbanks    fairbanks  0.379415\n",
       "ak                  ak  0.379415\n",
       "alaska          alaska  0.378358\n",
       "tongass        tongass  0.376191\n",
       "natives        natives  0.365874\n",
       "statehood    statehood  0.364736\n",
       "fish              fish  0.360768\n",
       "pipeline      pipeline  0.338675\n",
       "wildlife      wildlife  0.306912\n",
       "refuge          refuge  0.289692\n",
       "timber          timber  0.283745\n",
       "murkowski    murkowski  0.272302\n",
       "land              land  0.270684\n",
       "conveyance  conveyance  0.266318"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sparse_irrelevant(states['AK'])\n",
    "AK = states[\"AK\"]\n",
    "AK_words = wordlist(states['AK'])\n",
    "AK[AK_words].sum()\n",
    "AK_word_count = pd.DataFrame({'feature':AK_words, 'words':AK[AK_words].sum()}).sort_values(by='words',ascending=False)\n",
    "AK_word_count.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standarization\n",
    "with sklearn.preprocessing package\n",
    "## CENTERING SPARSE DATA... not!\n",
    "centering sparse data would destroy the sparseness structure in the data, but MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, specially if the features are in different scales.\n",
    "scale and StandardScaler can accept scipy.sparse matrices as input, as long as with_centering=False\n",
    "[More about this](http://scikit-learn.org/stable/modules/preprocessing.html#scaling-sparse-data)\n",
    "#### Small example on Alaska:\n",
    "I will normalize on one small subset of my data just to see what the results would be, how the values would change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>...</th>\n",
       "      <th>veterans</th>\n",
       "      <th>village</th>\n",
       "      <th>villages</th>\n",
       "      <th>water</th>\n",
       "      <th>whaling</th>\n",
       "      <th>wilderness</th>\n",
       "      <th>wildlife</th>\n",
       "      <th>wind</th>\n",
       "      <th>yeas</th>\n",
       "      <th>youth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Mark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Begich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>AK</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051424</td>\n",
       "      <td>0.091710</td>\n",
       "      <td>0.124813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>A.</td>\n",
       "      <td>Murkowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AK</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095305</td>\n",
       "      <td>0.129706</td>\n",
       "      <td>0.068622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085929</td>\n",
       "      <td>0.077309</td>\n",
       "      <td>0.085929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Ted</td>\n",
       "      <td>F.</td>\n",
       "      <td>Stevens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AK</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089775</td>\n",
       "      <td>0.080769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106864</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Don</td>\n",
       "      <td>E.</td>\n",
       "      <td>Young</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AK</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124897</td>\n",
       "      <td>0.082743</td>\n",
       "      <td>0.074442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 315 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     R title_x firstname middlename   lastname name_suffix nickname party_x  \\\n",
       "86   0     Sen      Mark        NaN     Begich         NaN      NaN       D   \n",
       "559  1     Sen      Lisa         A.  Murkowski         NaN      NaN       R   \n",
       "750  1     Sen       Ted         F.    Stevens         NaN      NaN       R   \n",
       "889  1     Rep       Don         E.      Young         NaN      NaN       R   \n",
       "\n",
       "    state_x   district_x    ...     veterans   village  villages     water  \\\n",
       "86       AK  Junior Seat    ...     0.051424  0.091710  0.124813  0.000000   \n",
       "559      AK  Senior Seat    ...     0.000000  0.095305  0.129706  0.068622   \n",
       "750      AK  Senior Seat    ...     0.000000  0.000000  0.000000  0.000000   \n",
       "889      AK            0    ...     0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      whaling wilderness  wildlife      wind      yeas     youth  \n",
       "86   0.000000   0.000000  0.074392  0.000000  0.000000  0.000000  \n",
       "559  0.000000   0.085929  0.077309  0.085929  0.000000  0.090092  \n",
       "750  0.000000   0.089775  0.080769  0.000000  0.106864  0.000000  \n",
       "889  0.124897   0.082743  0.074442  0.000000  0.000000  0.000000  \n",
       "\n",
       "[4 rows x 315 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.95716082  0.         ...,  1.          0.          1.        ]\n",
      " [ 1.          1.          1.         ...,  0.          1.          0.        ]\n",
      " [ 0.          0.92166899  0.         ...,  0.          0.          0.        ]]\n",
      "     absence     acres  agreed        ak    alaska  alaska's   alaskan  \\\n",
      "86       0.0  0.000000     0.0  0.962272  0.921049  0.921049  0.921049   \n",
      "559      0.0  0.957161     0.0  1.000000  0.957161  0.957161  0.957161   \n",
      "750      1.0  1.000000     1.0  0.000000  1.000000  1.000000  1.000000   \n",
      "889      0.0  0.921669     0.0  0.962920  0.921669  0.921669  0.921669   \n",
      "\n",
      "     alaskans  alcohol  allan  ...    veterans   village  villages  water  \\\n",
      "86   0.921049      0.0    1.0  ...         1.0  0.962272  0.962272    0.0   \n",
      "559  0.957161      1.0    0.0  ...         0.0  1.000000  1.000000    1.0   \n",
      "750  1.000000      0.0    0.0  ...         0.0  0.000000  0.000000    0.0   \n",
      "889  0.921669      0.0    0.0  ...         0.0  0.000000  0.000000    0.0   \n",
      "\n",
      "     whaling  wilderness  wildlife  wind  yeas  youth  \n",
      "86       0.0    0.000000  0.921049   0.0   0.0    0.0  \n",
      "559      0.0    0.957161  0.957161   1.0   0.0    1.0  \n",
      "750      0.0    1.000000  1.000000   0.0   1.0    0.0  \n",
      "889      1.0    0.921669  0.921669   0.0   0.0    0.0  \n",
      "\n",
      "[4 rows x 272 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import maxabs_scale\n",
    "print maxabs_scale(AK.ix[:,43:], axis=0, copy=False)\n",
    "AK.ix[:,43:] = maxabs_scale(AK.ix[:,43:], axis=0, copy=False)\n",
    "print AK.ix[:,43:].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the data is sacled without loosing it's sparese structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>...</th>\n",
       "      <th>veterans</th>\n",
       "      <th>village</th>\n",
       "      <th>villages</th>\n",
       "      <th>water</th>\n",
       "      <th>whaling</th>\n",
       "      <th>wilderness</th>\n",
       "      <th>wildlife</th>\n",
       "      <th>wind</th>\n",
       "      <th>yeas</th>\n",
       "      <th>youth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Mark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Begich</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>AK</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962272</td>\n",
       "      <td>0.962272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>A.</td>\n",
       "      <td>Murkowski</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AK</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957161</td>\n",
       "      <td>0.957161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Ted</td>\n",
       "      <td>F.</td>\n",
       "      <td>Stevens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AK</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Don</td>\n",
       "      <td>E.</td>\n",
       "      <td>Young</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AK</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.921669</td>\n",
       "      <td>0.921669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 315 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     R title_x firstname middlename   lastname name_suffix nickname party_x  \\\n",
       "86   0     Sen      Mark        NaN     Begich         NaN      NaN       D   \n",
       "559  1     Sen      Lisa         A.  Murkowski         NaN      NaN       R   \n",
       "750  1     Sen       Ted         F.    Stevens         NaN      NaN       R   \n",
       "889  1     Rep       Don         E.      Young         NaN      NaN       R   \n",
       "\n",
       "    state_x   district_x  ...   veterans   village  villages water whaling  \\\n",
       "86       AK  Junior Seat  ...        1.0  0.962272  0.962272   0.0     0.0   \n",
       "559      AK  Senior Seat  ...        0.0  1.000000  1.000000   1.0     0.0   \n",
       "750      AK  Senior Seat  ...        0.0  0.000000  0.000000   0.0     0.0   \n",
       "889      AK            0  ...        0.0  0.000000  0.000000   0.0     1.0   \n",
       "\n",
       "    wilderness  wildlife wind  yeas youth  \n",
       "86    0.000000  0.921049  0.0   0.0   0.0  \n",
       "559   0.957161  0.957161  1.0   0.0   1.0  \n",
       "750   1.000000  1.000000  0.0   1.0   0.0  \n",
       "889   0.921669  0.921669  0.0   0.0   0.0  \n",
       "\n",
       "[4 rows x 315 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL data scale\n",
    "### Method to make things easier not that I saw it working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_maxabsscaler(dataFrame,index):\n",
    "    dataFrame.ix[:,word_finder(dataFrame,index):] = maxabs_scale(dataFrame.ix[:,word_finder(dataFrame,index):], axis=0, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETED: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Gary</td>\n",
       "      <td>L.</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NY</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Robert</td>\n",
       "      <td>B.</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AL</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Kahikina</td>\n",
       "      <td>Akaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>A.</td>\n",
       "      <td>Allard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>CO</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   R title_x firstname middlename     lastname name_suffix nickname party_x  \\\n",
       "0  0     Rep      Neil        NaN  Abercrombie         NaN      NaN       D   \n",
       "1  0     Rep      Gary         L.     Ackerman         NaN      NaN       D   \n",
       "2  1     Rep    Robert         B.     Aderholt         NaN      NaN       R   \n",
       "3  0     Sen    Daniel   Kahikina        Akaka         NaN      NaN       D   \n",
       "4  1     Sen     Wayne         A.       Allard         NaN      NaN       R   \n",
       "\n",
       "  state_x   district_x ...   ziegler zimbabwe zimmer zinc zion zoberman zone  \\\n",
       "0      HI            1 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "1      NY            5 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "2      AL            4 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "3      HI  Junior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "4      CO  Senior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "\n",
       "  zones  zoo zuni  \n",
       "0   0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 14450 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_maxabsscaler(capitol_words,30)\n",
    "clean_sparse_irrelevant(capitol_words)\n",
    "capitol_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Decomposition and Dimensionality Reduction\n",
    "Just to compare result between these models in this particular data set.\n",
    "This is an example of when it's a good idea to reduce the number of columns in the data set. There are more than 14 000 columns (it was the resut of getting the words that were said the must as dummies and then getting the td-idf count of them)\n",
    "So too many columns are being used to predict the target variable, that is Republican or Democrat.\n",
    "One of the risks of these techniques is overfitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, LogisticRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R</th>\n",
       "      <th>title_x</th>\n",
       "      <th>firstname</th>\n",
       "      <th>middlename</th>\n",
       "      <th>lastname</th>\n",
       "      <th>name_suffix</th>\n",
       "      <th>nickname</th>\n",
       "      <th>party_x</th>\n",
       "      <th>state_x</th>\n",
       "      <th>district_x</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Neil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Gary</td>\n",
       "      <td>L.</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>NY</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Rep</td>\n",
       "      <td>Robert</td>\n",
       "      <td>B.</td>\n",
       "      <td>Aderholt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>AL</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Kahikina</td>\n",
       "      <td>Akaka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>HI</td>\n",
       "      <td>Junior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Sen</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>A.</td>\n",
       "      <td>Allard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "      <td>CO</td>\n",
       "      <td>Senior Seat</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   R title_x firstname middlename     lastname name_suffix nickname party_x  \\\n",
       "0  0     Rep      Neil        NaN  Abercrombie         NaN      NaN       D   \n",
       "1  0     Rep      Gary         L.     Ackerman         NaN      NaN       D   \n",
       "2  1     Rep    Robert         B.     Aderholt         NaN      NaN       R   \n",
       "3  0     Sen    Daniel   Kahikina        Akaka         NaN      NaN       D   \n",
       "4  1     Sen     Wayne         A.       Allard         NaN      NaN       R   \n",
       "\n",
       "  state_x   district_x ...   ziegler zimbabwe zimmer zinc zion zoberman zone  \\\n",
       "0      HI            1 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "1      NY            5 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "2      AL            4 ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "3      HI  Junior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "4      CO  Senior Seat ...       0.0      0.0    0.0  0.0  0.0      0.0  0.0   \n",
       "\n",
       "  zones  zoo zuni  \n",
       "0   0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 14450 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitol_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  where words start being useful\n",
    "After normalization, some words had a global weight that was very small in a td-idf matrix count, so their column.sum() was cero, I will not feed that to my model because a colum n filled with 0 will not add much variance in a spacer matrix. Also at index 30 is where I the sparse matrix got attached to the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a&amp;m</th>\n",
       "      <th>a-plus</th>\n",
       "      <th>a.</th>\n",
       "      <th>a.d.</th>\n",
       "      <th>a.m.</th>\n",
       "      <th>a.m.e.</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aacute</th>\n",
       "      <th>aahsa</th>\n",
       "      <th>aamodt</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13643 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a&m  a-plus   a.  a.d.      a.m.  a.m.e.  aaa  aacute  aahsa  aamodt  ...   \\\n",
       "0  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "1  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "2  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "3  0.0     0.0  0.0   0.0  0.000000     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "4  0.0     0.0  0.0   0.0  0.096384     0.0  0.0     0.0    0.0     0.0  ...    \n",
       "\n",
       "   ziegler  zimbabwe  zimmer  zinc  zion  zoberman  zone  zones  zoo  zuni  \n",
       "0      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "1      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "2      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "3      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "4      0.0       0.0     0.0   0.0   0.0       0.0   0.0    0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 13643 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capitol_words[word_column_names_capitol].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abernathy</th>\n",
       "      <th>abilene</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>abilityone</th>\n",
       "      <th>abington</th>\n",
       "      <th>able</th>\n",
       "      <th>able-bodied</th>\n",
       "      <th>abm</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegler</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aberdeen</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.003932</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.005992</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.003210</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>-0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abernathy</th>\n",
       "      <td>-0.002268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.002275</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abilene</th>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.002275</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abilities</th>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.002275</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>-0.002268</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002275</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.447163</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>-0.001856</td>\n",
       "      <td>-0.001312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aberdeen  abernathy   abilene  abilities   ability  abilityone  \\\n",
       "aberdeen   1.000000  -0.002268 -0.002268  -0.002268 -0.002268   -0.003932   \n",
       "abernathy -0.002268   1.000000 -0.001312  -0.001312 -0.001312   -0.002275   \n",
       "abilene   -0.002268  -0.001312  1.000000  -0.001312 -0.001312   -0.002275   \n",
       "abilities -0.002268  -0.001312 -0.001312   1.000000 -0.001312   -0.002275   \n",
       "ability   -0.002268  -0.001312 -0.001312  -0.001312  1.000000   -0.002275   \n",
       "\n",
       "           abington      able  able-bodied       abm    ...      ziegler  \\\n",
       "aberdeen  -0.002268 -0.005992    -0.002268 -0.002268    ...    -0.002268   \n",
       "abernathy -0.001312 -0.003467    -0.001312 -0.001312    ...    -0.001312   \n",
       "abilene   -0.001312 -0.003467    -0.001312 -0.001312    ...    -0.001312   \n",
       "abilities -0.001312 -0.003467    -0.001312 -0.001312    ...    -0.001312   \n",
       "ability   -0.001312  0.447163    -0.001312 -0.001312    ...    -0.001312   \n",
       "\n",
       "           zimbabwe    zimmer      zinc      zion  zoberman      zone  \\\n",
       "aberdeen  -0.003210 -0.002268 -0.002268 -0.002268 -0.003209 -0.003198   \n",
       "abernathy -0.001857 -0.001312 -0.001312 -0.001312 -0.001856 -0.001850   \n",
       "abilene   -0.001857 -0.001312 -0.001312 -0.001312 -0.001856 -0.001850   \n",
       "abilities -0.001857 -0.001312 -0.001312 -0.001312 -0.001856 -0.001850   \n",
       "ability   -0.001857 -0.001312 -0.001312 -0.001312 -0.001856 -0.001850   \n",
       "\n",
       "              zones       zoo      zuni  \n",
       "aberdeen  -0.002268 -0.003208 -0.002268  \n",
       "abernathy -0.001312 -0.001856 -0.001312  \n",
       "abilene   -0.001312 -0.001856 -0.001312  \n",
       "abilities -0.001312 -0.001856 -0.001312  \n",
       "ability   -0.001312 -0.001856 -0.001312  \n",
       "\n",
       "[5 rows x 13614 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_correlations = capitol_words.ix[:,836:].corr()\n",
    "global_correlations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.plt.figure(figsize=(24,20))\n",
    "sns.heatmap(capitol_words.ix[:,836:].transpose().corr().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCOMPONENT MATRIX:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "      <th>component_3</th>\n",
       "      <th>component_4</th>\n",
       "      <th>component_5</th>\n",
       "      <th>component_6</th>\n",
       "      <th>component_7</th>\n",
       "      <th>component_8</th>\n",
       "      <th>component_9</th>\n",
       "      <th>component_10</th>\n",
       "      <th>...</th>\n",
       "      <th>component_754</th>\n",
       "      <th>component_755</th>\n",
       "      <th>component_756</th>\n",
       "      <th>component_757</th>\n",
       "      <th>component_758</th>\n",
       "      <th>component_759</th>\n",
       "      <th>component_760</th>\n",
       "      <th>component_761</th>\n",
       "      <th>component_762</th>\n",
       "      <th>component_763</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.224480</td>\n",
       "      <td>-0.452334</td>\n",
       "      <td>0.113147</td>\n",
       "      <td>-0.837146</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>-0.449614</td>\n",
       "      <td>0.953103</td>\n",
       "      <td>0.527301</td>\n",
       "      <td>-0.294541</td>\n",
       "      <td>0.106687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>-0.007764</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>-0.131950</td>\n",
       "      <td>0.092108</td>\n",
       "      <td>-0.054300</td>\n",
       "      <td>0.046542</td>\n",
       "      <td>0.046614</td>\n",
       "      <td>0.015238</td>\n",
       "      <td>-1.491324e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122874</td>\n",
       "      <td>-1.617364</td>\n",
       "      <td>1.173446</td>\n",
       "      <td>-1.117909</td>\n",
       "      <td>-2.904385</td>\n",
       "      <td>0.645005</td>\n",
       "      <td>1.099246</td>\n",
       "      <td>-0.098900</td>\n",
       "      <td>2.621183</td>\n",
       "      <td>-0.296297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232765</td>\n",
       "      <td>-0.124079</td>\n",
       "      <td>-0.034230</td>\n",
       "      <td>0.058049</td>\n",
       "      <td>0.071679</td>\n",
       "      <td>-0.168641</td>\n",
       "      <td>-0.036262</td>\n",
       "      <td>0.093936</td>\n",
       "      <td>0.011447</td>\n",
       "      <td>-1.491324e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.881820</td>\n",
       "      <td>-0.661854</td>\n",
       "      <td>-0.793238</td>\n",
       "      <td>-0.685070</td>\n",
       "      <td>0.039017</td>\n",
       "      <td>-0.544605</td>\n",
       "      <td>-0.204850</td>\n",
       "      <td>0.636111</td>\n",
       "      <td>-0.647944</td>\n",
       "      <td>-0.950342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076137</td>\n",
       "      <td>-0.061312</td>\n",
       "      <td>0.070325</td>\n",
       "      <td>-0.050951</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>-0.000775</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>-0.012703</td>\n",
       "      <td>-1.491324e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.326928</td>\n",
       "      <td>0.179372</td>\n",
       "      <td>0.537611</td>\n",
       "      <td>-1.262296</td>\n",
       "      <td>1.292680</td>\n",
       "      <td>0.818766</td>\n",
       "      <td>0.232159</td>\n",
       "      <td>0.280342</td>\n",
       "      <td>-0.265586</td>\n",
       "      <td>0.209218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047391</td>\n",
       "      <td>0.086163</td>\n",
       "      <td>-0.022755</td>\n",
       "      <td>-0.275561</td>\n",
       "      <td>0.241222</td>\n",
       "      <td>0.044327</td>\n",
       "      <td>-0.086261</td>\n",
       "      <td>0.037930</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>-1.491324e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.580614</td>\n",
       "      <td>1.916924</td>\n",
       "      <td>-0.533527</td>\n",
       "      <td>-2.672486</td>\n",
       "      <td>-0.042675</td>\n",
       "      <td>-0.268796</td>\n",
       "      <td>0.374946</td>\n",
       "      <td>0.538247</td>\n",
       "      <td>0.395623</td>\n",
       "      <td>0.483640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>-0.170579</td>\n",
       "      <td>-0.140365</td>\n",
       "      <td>-0.334942</td>\n",
       "      <td>-0.070205</td>\n",
       "      <td>0.228099</td>\n",
       "      <td>0.104589</td>\n",
       "      <td>-0.003303</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>-1.491324e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 763 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   component_1  component_2  component_3  component_4  component_5  \\\n",
       "0    -0.224480    -0.452334     0.113147    -0.837146     0.221519   \n",
       "1     0.122874    -1.617364     1.173446    -1.117909    -2.904385   \n",
       "2    -0.881820    -0.661854    -0.793238    -0.685070     0.039017   \n",
       "3     1.326928     0.179372     0.537611    -1.262296     1.292680   \n",
       "4     2.580614     1.916924    -0.533527    -2.672486    -0.042675   \n",
       "\n",
       "   component_6  component_7  component_8  component_9  component_10  \\\n",
       "0    -0.449614     0.953103     0.527301    -0.294541      0.106687   \n",
       "1     0.645005     1.099246    -0.098900     2.621183     -0.296297   \n",
       "2    -0.544605    -0.204850     0.636111    -0.647944     -0.950342   \n",
       "3     0.818766     0.232159     0.280342    -0.265586      0.209218   \n",
       "4    -0.268796     0.374946     0.538247     0.395623      0.483640   \n",
       "\n",
       "       ...        component_754  component_755  component_756  component_757  \\\n",
       "0      ...             0.031183      -0.007764       0.009641      -0.131950   \n",
       "1      ...            -0.232765      -0.124079      -0.034230       0.058049   \n",
       "2      ...             0.076137      -0.061312       0.070325      -0.050951   \n",
       "3      ...             0.047391       0.086163      -0.022755      -0.275561   \n",
       "4      ...             0.018014      -0.170579      -0.140365      -0.334942   \n",
       "\n",
       "   component_758  component_759  component_760  component_761  component_762  \\\n",
       "0       0.092108      -0.054300       0.046542       0.046614       0.015238   \n",
       "1       0.071679      -0.168641      -0.036262       0.093936       0.011447   \n",
       "2       0.062893      -0.000775       0.005773       0.035851      -0.012703   \n",
       "3       0.241222       0.044327      -0.086261       0.037930       0.002989   \n",
       "4      -0.070205       0.228099       0.104589      -0.003303       0.016562   \n",
       "\n",
       "   component_763  \n",
       "0  -1.491324e-14  \n",
       "1  -1.491324e-14  \n",
       "2  -1.491324e-14  \n",
       "3  -1.491324e-14  \n",
       "4  -1.491324e-14  \n",
       "\n",
       "[5 rows x 763 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "transformed_pca_x = pca.fit_transform(capitol_words[word_column_names_capitol])\n",
    "component_names = [\"component_\"+str(comp) for comp in range(1, len(pca.explained_variance_)+1)]\n",
    "transformed_pca_x = pd.DataFrame(transformed_pca_x,columns=component_names)\n",
    "print \"CCOMPONENT MATRIX:\"\n",
    "transformed_pca_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "component_matrix = pd.DataFrame(pca.components_,index=component_names, columns=word_column_names_capitol)\n",
    "component_matrix[\"explained_variance_ratio\"] = pca.explained_variance_ratio_\n",
    "component_matrix[\"eigenvalue\"] = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Matrix:\n",
    "#### The reslt is not the easiest for intepretation\n",
    "The problem with this is that PCA expects features with little to no correlation, and in this case, with words if I were to build a model that was based on eliminating similar words or correlated words, this would only acomplish the task of being overfitted and it would not do well at all for predicting a real example.\n",
    "Let's say one of the components was based on the word \"small\" and \"small\" is correlated with \"little\" but I just deleted little. Unless I have another way to capture semantic similarity I can't get rid of those words just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a&amp;m</th>\n",
       "      <th>a-plus</th>\n",
       "      <th>a.</th>\n",
       "      <th>a.d.</th>\n",
       "      <th>a.m.</th>\n",
       "      <th>a.m.e.</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aacute</th>\n",
       "      <th>aahsa</th>\n",
       "      <th>aamodt</th>\n",
       "      <th>...</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zoberman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuni</th>\n",
       "      <th>explained_variance_ratio</th>\n",
       "      <th>eigenvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-8.481141e-05</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>-0.000547</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>-0.000571</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>1.286851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>-1.188319e-04</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000548</td>\n",
       "      <td>-0.000281</td>\n",
       "      <td>-0.000921</td>\n",
       "      <td>-0.002143</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>-0.002575</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>0.014102</td>\n",
       "      <td>0.984469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_3</th>\n",
       "      <td>-0.000323</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>-3.389893e-05</td>\n",
       "      <td>-0.001907</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.001189</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>-0.000840</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.877171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_4</th>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>9.861516e-07</td>\n",
       "      <td>-0.006105</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>-0.001064</td>\n",
       "      <td>-0.000860</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.001854</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.759831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_5</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>-7.284733e-06</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.001720</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>0.610327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 13645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  a&m    a-plus        a.          a.d.      a.m.    a.m.e.  \\\n",
       "component_1 -0.000263 -0.000040  0.000200 -8.481141e-05  0.005106 -0.000146   \n",
       "component_2 -0.000144  0.000042 -0.000273 -1.188319e-04  0.004800 -0.000162   \n",
       "component_3 -0.000323 -0.000190 -0.000317 -3.389893e-05 -0.001907 -0.000012   \n",
       "component_4 -0.000046 -0.000130  0.000461  9.861516e-07 -0.006105 -0.000030   \n",
       "component_5  0.000008  0.000182 -0.000646 -7.284733e-06  0.002609  0.000077   \n",
       "\n",
       "                  aaa    aacute     aahsa    aamodt     ...        zimmer  \\\n",
       "component_1  0.000068  0.000121 -0.000081 -0.000125     ...     -0.000983   \n",
       "component_2  0.000053 -0.000150 -0.000072 -0.000062     ...     -0.000548   \n",
       "component_3 -0.000150  0.000133 -0.000070 -0.000274     ...     -0.001036   \n",
       "component_4  0.000146 -0.000312  0.000037  0.000049     ...     -0.000098   \n",
       "component_5  0.000024 -0.000637  0.000047  0.000305     ...      0.000136   \n",
       "\n",
       "                 zinc      zion  zoberman      zone     zones       zoo  \\\n",
       "component_1 -0.000561 -0.000547 -0.000827  0.000844  0.000715 -0.001128   \n",
       "component_2 -0.000281 -0.000921 -0.002143 -0.000024  0.000873 -0.002575   \n",
       "component_3 -0.000881 -0.000120 -0.000150 -0.001189 -0.000788  0.000744   \n",
       "component_4  0.000483 -0.001064 -0.000860 -0.001667  0.001344  0.000019   \n",
       "component_5 -0.001080 -0.000989 -0.001720 -0.001229  0.000469  0.001366   \n",
       "\n",
       "                 zuni  explained_variance_ratio  eigenvalue  \n",
       "component_1 -0.000571                  0.018433    1.286851  \n",
       "component_2 -0.000506                  0.014102    0.984469  \n",
       "component_3 -0.000840                  0.012565    0.877171  \n",
       "component_4 -0.001854                  0.010884    0.759831  \n",
       "component_5  0.002696                  0.008742    0.610327  \n",
       "\n",
       "[5 rows x 13645 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of LR model:  0.655021834061\n"
     ]
    }
   ],
   "source": [
    "X = transformed_pca_x.ix[:,:500]\n",
    "y = capitol_words[\"R\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "lr = LogisticRegression(C=1e9, penalty='l1')\n",
    "lr.fit(X_train,y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print \"Test set accuracy of LR model: \",metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Low benefit\n",
    "Now with all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of LR model:  0.908296943231\n"
     ]
    }
   ],
   "source": [
    "X = capitol_words[word_column_names_capitol]\n",
    "y = capitol_words[\"R\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "\n",
    "lr = LogisticRegression(C=1e9, penalty='l2')\n",
    "lr.fit(X_train,y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print \"Test set accuracy of LR model: \",metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy on the test set:  0.524017467249\n"
     ]
    }
   ],
   "source": [
    "print \"Null accuracy on the test set: \",y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent class dummy classifier test accuracy:  0.524017467249\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dumb_model = DummyClassifier(strategy='most_frequent')\n",
    "dumb_model.fit(X_train, y_train)\n",
    "y_dumb_class = dumb_model.predict(X_test)\n",
    "print \"Most frequent class dummy classifier test accuracy: \",metrics.accuracy_score(y_test, y_dumb_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross val scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515185185185\n",
      "0.907283950617\n",
      "0.0573276823972\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAECCAYAAADq7fyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d8976d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dumb_model = DummyClassifier(strategy='most_frequent')\n",
    "dummy_scores = cross_val_score(dumb_model, X, y, cv=30)\n",
    "real_scores = cross_val_score(LogisticRegression(),X , y,cv=30)\n",
    "sns.plt.hist(dummy_scores)\n",
    "sns.plt.hist(real_scores)\n",
    "#we could use a cv=Startifield Kfold for when you have really unbalanced\n",
    "#real_scores = cross_val_score(LogisticRegression(),X , y,cv=30)\n",
    "print np.mean(dummy_scores)\n",
    "print np.mean(real_scores)\n",
    "print np.std(real_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 99  10]\n",
      " [ 11 109]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13db00d90>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD9CAYAAAAf46TtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDF1JKsjQlaSC/g+0KJEgjAqFWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x148e7cf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print cm\n",
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity/Recall (TPR):  0.908333333333\n",
      "Precision (PPV):  0.915966386555\n",
      "NPV:  0.9\n",
      "Accuracy:  0.908296943231\n",
      "F1: 0.912133891213\n"
     ]
    }
   ],
   "source": [
    "print \"Sensitivity/Recall (TPR): \",metrics.recall_score(y_test,y_test_pred)\n",
    "print \"Precision (PPV): \", metrics.precision_score(y_test,y_test_pred)\n",
    "print \"NPV: \", cm[0,0] / float(cm[0,0]+cm[1,0])\n",
    "print \"Accuracy: \", metrics.accuracy_score(y_test,y_test_pred)\n",
    "print \"F1:\", metrics.f1_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.91      0.90       109\n",
      "          1       0.92      0.91      0.91       120\n",
      "\n",
      "avg / total       0.91      0.91      0.91       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Classification Report:\\n\", metrics.classification_report(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression predicted probabilities for first five samples in test set:\n",
      "[[ 0.966  0.034]\n",
      " [ 1.     0.   ]\n",
      " [ 0.338  0.662]\n",
      " [ 0.998  0.002]\n",
      " [ 0.183  0.817]]\n",
      "Logistic Regression predictions for first five samples in test set:\n",
      "[0 0 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.338</td>\n",
       "      <td>0.662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.183</td>\n",
       "      <td>0.817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_0  class_1  predicted  actual\n",
       "0    0.966    0.034        0.0     0.0\n",
       "1    1.000    0.000        0.0     0.0\n",
       "2    0.338    0.662        1.0     1.0\n",
       "3    0.998    0.002        0.0     0.0\n",
       "4    0.183    0.817        1.0     0.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr probabilities per category for first five samples\n",
    "predicted_probs_lr = lr.predict_proba(X_test).round(3)\n",
    "predictions_lr = lr.predict(X_test)\n",
    "\n",
    "print \"Logistic Regression predicted probabilities for first five samples in test set:\\n\",predicted_probs_lr[:5]\n",
    "print \"Logistic Regression predictions for first five samples in test set:\\n\",predictions_lr[:5]\n",
    "y_test_lr_df = pd.DataFrame(\n",
    "    np.concatenate((\n",
    "        predicted_probs_lr,predictions_lr.reshape((predictions_lr.shape[0],-1)),\n",
    "        y_test.reshape((y_test.shape[0],-1))),axis=1\n",
    "    ),\n",
    "    columns = [\"class_0\",\"class_1\",\"predicted\",\"actual\"])\n",
    "\n",
    "y_test_lr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class_0  class_1  predicted  actual\n",
      "46     0.983    0.017        0.0     1.0\n",
      "50     0.942    0.058        0.0     1.0\n",
      "55     0.955    0.045        0.0     1.0\n",
      "176    0.992    0.008        0.0     1.0\n",
      "     class_0  class_1  predicted  actual\n",
      "76     0.063    0.937        1.0     0.0\n",
      "128    0.067    0.933        1.0     0.0\n",
      "150    0.055    0.945        1.0     0.0\n",
      "222    0.000    1.000        1.0     0.0\n"
     ]
    }
   ],
   "source": [
    "bad_y_class_0 = y_test_lr_df[np.logical_and(y_test_lr_df.class_0>.9, y_test_lr_df.actual==1.0)]\n",
    "print bad_y_class_0\n",
    "bad_y_class_1 = y_test_lr_df[np.logical_and(y_test_lr_df.class_1>.9, y_test_lr_df.actual==0.0)]\n",
    "print bad_y_class_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_0  class_1  predicted  actual\n",
       "0     0.70     0.30        0.0     0.0\n",
       "1     0.75     0.25        0.0     0.0\n",
       "2     0.28     0.72        1.0     1.0\n",
       "3     0.67     0.33        0.0     0.0\n",
       "4     0.65     0.35        0.0     0.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "predicted_probs_rf = rf.predict_proba(X_test)\n",
    "predictions_rf = rf.predict(X_test)\n",
    "\n",
    "y_test_rf_df = pd.DataFrame(\n",
    "    np.concatenate((\n",
    "        predicted_probs_rf,predictions_rf.reshape((predictions_rf.shape[0],-1)),\n",
    "        y_test.reshape((y_test.shape[0],-1))),axis=1\n",
    "    ),\n",
    "    columns = [\"class_0\",\"class_1\",\"predicted\",\"actual\"])\n",
    "\n",
    "y_test_rf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x145b70bd0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEMCAYAAADeYiHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x148f5ea50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate lr model false positive and true positive rates\n",
    "fpr_lr, tpr_lr, thresholds_lr = metrics.roc_curve(y_test, predicted_probs_lr[:,1])\n",
    "\n",
    "#generate same for random forest model\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(y_test, predicted_probs_rf[:,1])\n",
    "\n",
    "# plot LR and RF model ROC curves\n",
    "sns.plt.plot(fpr_lr, tpr_lr,label=\"lr\")\n",
    "sns.plt.plot(fpr_rf, tpr_rf,label=\"rf\")\n",
    "sns.plt.xlim([0, 1])\n",
    "sns.plt.ylim([0, 1.05])\n",
    "sns.plt.legend(loc=\"lower right\")\n",
    "sns.plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model AUC:  0.96498470948\n",
      "RF model AUC:  0.919762996942\n"
     ]
    }
   ],
   "source": [
    "# calculate AUC for lr and rf\n",
    "print \"LR model AUC: \",metrics.roc_auc_score(y_test, predicted_probs_lr[:,1])\n",
    "print \"RF model AUC: \",metrics.roc_auc_score(y_test, predicted_probs_rf[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x13e6d9710>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAFECAYAAAA9aanpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x148e1dd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot LR and RF model ROC curves\n",
    "sns.plt.plot(fpr_lr, tpr_lr,label=\"lr\")\n",
    "sns.plt.plot(fpr_lr,thresholds_lr, label=\"lr_thresh\")\n",
    "sns.plt.xlim([0, 1])\n",
    "sns.plt.ylim([0, 1.05])\n",
    "sns.plt.legend(loc=\"center\")\n",
    "sns.plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.plt.ylabel('True Positive Rate (Sensitivity) or Class 1 Threshold Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_0  class_1  predicted  actual  predicted_075  predicted_072\n",
      "0    0.966    0.034        0.0     0.0            0.0            0.0\n",
      "1    1.000    0.000        0.0     0.0            0.0            0.0\n",
      "2    0.338    0.662        1.0     1.0            0.0            0.0\n",
      "3    0.998    0.002        0.0     0.0            0.0            0.0\n",
      "4    0.183    0.817        1.0     0.0            1.0            1.0\n",
      "Confusion matrix at original 0.5 threshold:\n",
      "[[ 99  10]\n",
      " [ 11 109]] \n",
      "\n",
      "Classification Report at original 0.5 threshold:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.91      0.90       109\n",
      "        1.0       0.92      0.91      0.91       120\n",
      "\n",
      "avg / total       0.91      0.91      0.91       229\n",
      "\n",
      "\n",
      "Confusion matrix at 0.72 threshold:\n",
      "[[102   7]\n",
      " [ 15 105]] \n",
      "\n",
      "Classification Report at 0.72 threshold:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.87      0.94      0.90       109\n",
      "        1.0       0.94      0.88      0.91       120\n",
      "\n",
      "avg / total       0.91      0.90      0.90       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_lr_df[\"predicted_072\"] = (y_test_lr_df.class_1 > 0.72).astype(float)\n",
    "print y_test_lr_df.head()\n",
    "print \"Confusion matrix at original 0.5 threshold:\\n\",metrics.confusion_matrix(y_test_lr_df.actual,\n",
    "                                                                      y_test_lr_df.predicted),\"\\n\"\n",
    "print \"Classification Report at original 0.5 threshold:\\n\", metrics.classification_report(y_test_lr_df.actual,\n",
    "                                                                                          y_test_lr_df.predicted),\"\\n\"\n",
    "print \"Confusion matrix at 0.72 threshold:\\n\",metrics.confusion_matrix(y_test_lr_df.actual,\n",
    "                                                                      y_test_lr_df.predicted_072),\"\\n\"\n",
    "print \"Classification Report at 0.72 threshold:\\n\", metrics.classification_report(y_test_lr_df.actual,\n",
    "                                                                                 y_test_lr_df.predicted_072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong way to calculate LR model AUC:  0.908295107034\n",
      "Wrong way to calculate RF model AUC:  0.837423547401\n"
     ]
    }
   ],
   "source": [
    "# calculate AUC using y_pred_class (producing incorrect results)\n",
    "print \"Wrong way to calculate LR model AUC: \",metrics.roc_auc_score(y_test, predictions_lr)\n",
    "print \"Wrong way to calculate RF model AUC: \",metrics.roc_auc_score(y_test, predictions_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x146bd1150>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x146a37350>], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoIkqfhfMLg73gN9fQQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x145b836d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146bdb110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of predicted probabilities grouped by actual response value for LR\n",
    "y_test_lr_df.class_1.hist(by= y_test_lr_df.actual, sharex=True, sharey=True)\n",
    "#same for RF\n",
    "y_test_rf_df.class_1.hist(by= y_test_rf_df.actual, sharex=True, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of default 0.5 threshold LR model:  0.908296943231\n"
     ]
    }
   ],
   "source": [
    "#convert outcome into binary 0/1 attribute\n",
    "le = LabelEncoder()\n",
    "#create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "#create logistic regression object\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print \"Test set accuracy of default 0.5 threshold LR model: \",metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 46.,  19.,  21.,  12.,  14.,   8.,  13.,  12.,  22.,  62.]),\n",
       " array([  6.34175732e-04,   1.00409657e-01,   2.00185139e-01,\n",
       "          2.99960621e-01,   3.99736103e-01,   4.99511584e-01,\n",
       "          5.99287066e-01,   6.99062548e-01,   7.98838030e-01,\n",
       "          8.98613511e-01,   9.98388993e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAECCAYAAADq7fyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e2adbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate predicted probabilities for class 1\n",
    "y_pred_prob1 = lr.predict_proba(X_test)[:, 1]\n",
    "# show predicted probabilities in a histogram\n",
    "sns.plt.hist(y_pred_prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97048929663608563"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x147ab8f90>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEMCAYAAADeYiHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7NCThBmZlbo/wGnXWBO1YRh3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x148b24b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob1)\n",
    "sns.plt.plot(fpr, tpr)\n",
    "sns.plt.xlim([0, 1])\n",
    "sns.plt.ylim([0, 1.05])\n",
    "sns.plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "sns.plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef = lr.coef_\n",
    "word_column_names_capitolcoeficient_weight = pd.DataFrame({'coeficient':coef[0], 'words':word_column_names_capitol}).sort_values(by='coeficient',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coeficient</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10505</th>\n",
       "      <td>1.144067</td>\n",
       "      <td>requesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>0.951347</td>\n",
       "      <td>entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12182</th>\n",
       "      <td>0.678557</td>\n",
       "      <td>taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3577</th>\n",
       "      <td>0.636294</td>\n",
       "      <td>description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>0.598514</td>\n",
       "      <td>spending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7288</th>\n",
       "      <td>0.523187</td>\n",
       "      <td>liability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12280</th>\n",
       "      <td>0.514491</td>\n",
       "      <td>terrorists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.471373</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11609</th>\n",
       "      <td>0.471207</td>\n",
       "      <td>speaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>0.430632</td>\n",
       "      <td>stimulus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>0.410192</td>\n",
       "      <td>earmarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8814</th>\n",
       "      <td>0.410131</td>\n",
       "      <td>obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>0.408883</td>\n",
       "      <td>consume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12177</th>\n",
       "      <td>0.386378</td>\n",
       "      <td>tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9196</th>\n",
       "      <td>0.383281</td>\n",
       "      <td>paragraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.354131</td>\n",
       "      <td>account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12277</th>\n",
       "      <td>0.342442</td>\n",
       "      <td>terror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9905</th>\n",
       "      <td>0.331774</td>\n",
       "      <td>products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.330883</td>\n",
       "      <td>aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>0.323672</td>\n",
       "      <td>barrels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>0.310474</td>\n",
       "      <td>production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>0.306203</td>\n",
       "      <td>illegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>0.287868</td>\n",
       "      <td>doesn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>0.283224</td>\n",
       "      <td>flexibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10226</th>\n",
       "      <td>0.279909</td>\n",
       "      <td>reagan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12624</th>\n",
       "      <td>0.278136</td>\n",
       "      <td>trillion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>0.274798</td>\n",
       "      <td>energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9290</th>\n",
       "      <td>0.272049</td>\n",
       "      <td>patrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11186</th>\n",
       "      <td>0.271356</td>\n",
       "      <td>sergeant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coeficient        words\n",
       "10505    1.144067   requesting\n",
       "4356     0.951347       entity\n",
       "12182    0.678557        taxes\n",
       "3577     0.636294  description\n",
       "11635    0.598514     spending\n",
       "7288     0.523187    liability\n",
       "12280    0.514491   terrorists\n",
       "41       0.471373     abortion\n",
       "11609    0.471207      speaker\n",
       "11826    0.430632     stimulus\n",
       "4073     0.410192     earmarks\n",
       "8814     0.410131        obama\n",
       "2865     0.408883      consume\n",
       "12177    0.386378          tax\n",
       "9196     0.383281    paragraph\n",
       "84       0.354131      account\n",
       "12277    0.342442       terror\n",
       "9905     0.331774     products\n",
       "404      0.330883       aliens\n",
       "1114     0.323672      barrels\n",
       "9903     0.310474   production\n",
       "6228     0.306203      illegal\n",
       "3864     0.287868      doesn't\n",
       "4908     0.283224  flexibility\n",
       "10226    0.279909       reagan\n",
       "12624    0.278136     trillion\n",
       "4300     0.274798       energy\n",
       "9290     0.272049       patrol\n",
       "11186    0.271356     sergeant"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mask(coeficient_weight,\"coeficient\",\"<=\",0.2).shape[0]\n",
    "my_mask(coeficient_weight,\"coeficient\",\">=\",0.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    393\n",
       "D    367\n",
       "I      3\n",
       "Name: party_x, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vertebral_data.outcome.value_counts\n",
    "capitol_words.party_x.value_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x147cb0910>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABAwAAAGpCAYAAAAJCqSMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABA4/8H7iMHrZ/xfYMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147987150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(capitol_words,x_vars=[\"requesting\",\"entity\",\"taxes\"],y_vars=\"R\", size=6, aspect=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome_pred_class_log = lr.predict(X)\n",
    "outcome_pred_class_log.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x145cb5410>]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAECCAYAAAD5OrxGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x145ca1c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the class predictions\n",
    "capitol_words.sort('R', inplace=True)\n",
    "plt.scatter(capitol_words.requesting, capitol_words.R)\n",
    "plt.plot(capitol_words.requesting, outcome_pred_class_log, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coeficient</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11623</th>\n",
       "      <td>-0.304852</td>\n",
       "      <td>speculation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>-0.306730</td>\n",
       "      <td>jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>-0.307061</td>\n",
       "      <td>coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>-0.307973</td>\n",
       "      <td>coverage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13498</th>\n",
       "      <td>-0.308336</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>-0.319878</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.324439</td>\n",
       "      <td>afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>-0.327718</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>-0.330338</td>\n",
       "      <td>profits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>-0.333728</td>\n",
       "      <td>civil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>-0.335763</td>\n",
       "      <td>amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>-0.337019</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>-0.344415</td>\n",
       "      <td>foreclosure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6448</th>\n",
       "      <td>-0.357578</td>\n",
       "      <td>insurance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11512</th>\n",
       "      <td>-0.362909</td>\n",
       "      <td>solar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>-0.365112</td>\n",
       "      <td>dogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9143</th>\n",
       "      <td>-0.366890</td>\n",
       "      <td>pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9313</th>\n",
       "      <td>-0.379126</td>\n",
       "      <td>paygo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>-0.385722</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>-0.390771</td>\n",
       "      <td>college</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10129</th>\n",
       "      <td>-0.406035</td>\n",
       "      <td>rail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236</th>\n",
       "      <td>-0.418169</td>\n",
       "      <td>cuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>-0.431469</td>\n",
       "      <td>rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.446720</td>\n",
       "      <td>african</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>-0.448431</td>\n",
       "      <td>companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>-0.486173</td>\n",
       "      <td>republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>-0.505536</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13012</th>\n",
       "      <td>-0.587297</td>\n",
       "      <td>veterans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>-0.592364</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6574</th>\n",
       "      <td>-0.728009</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coeficient        words\n",
       "11623   -0.304852  speculation\n",
       "6646    -0.306730         jack\n",
       "2536    -0.307061        coast\n",
       "3061    -0.307973     coverage\n",
       "13498   -0.308336        would\n",
       "4140    -0.319878    education\n",
       "241     -0.324439  afghanistan\n",
       "1390    -0.327718        black\n",
       "9917    -0.330338      profits\n",
       "2399    -0.333728        civil\n",
       "488     -0.335763    amendment\n",
       "3865    -0.337019          dog\n",
       "4986    -0.344415  foreclosure\n",
       "6448    -0.357578    insurance\n",
       "11512   -0.362909        solar\n",
       "3866    -0.365112         dogs\n",
       "9143    -0.366890      pacific\n",
       "9313    -0.379126        paygo\n",
       "1445    -0.385722         blue\n",
       "2594    -0.390771      college\n",
       "10129   -0.406035         rail\n",
       "3236    -0.418169         cuts\n",
       "10659   -0.431469       rights\n",
       "246     -0.446720      african\n",
       "2702    -0.448431    companies\n",
       "10499   -0.486173   republican\n",
       "11496   -0.505536       social\n",
       "13012   -0.587297     veterans\n",
       "1828    -0.592364         bush\n",
       "6574    -0.728009         iraq"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mask(coeficient_weight,\"coeficient\",\"<=\",-.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>odessa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offenders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9111</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9112</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9113</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9114</th>\n",
       "      <td>0.0</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9115</th>\n",
       "      <td>0.0</td>\n",
       "      <td>officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9116</th>\n",
       "      <td>0.0</td>\n",
       "      <td>officers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9117</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>official</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>officials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9120</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offsets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>offshore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>0.0</td>\n",
       "      <td>often</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9124</th>\n",
       "      <td>0.0</td>\n",
       "      <td>oftentimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ogden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9128</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ohioans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ohkay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>oetken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>october</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9078</th>\n",
       "      <td>0.0</td>\n",
       "      <td>observation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ocs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9080</th>\n",
       "      <td>0.0</td>\n",
       "      <td>observatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>0.0</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eaton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earmarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earmarking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>0.0</td>\n",
       "      <td>e15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dyas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dyersburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dyess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4545</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dylan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dynamic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dynamics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dyslexia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dystonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>dystrophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4552</th>\n",
       "      <td>0.0</td>\n",
       "      <td>e85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earmarked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>0.0</td>\n",
       "      <td>each</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4557</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eagle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eagles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>0.0</td>\n",
       "      <td>early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>0.0</td>\n",
       "      <td>earmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13669</th>\n",
       "      <td>0.0</td>\n",
       "      <td>zuni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13670 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       counts        words\n",
       "0         0.0           00\n",
       "9105      0.0       odessa\n",
       "9107      0.0           of\n",
       "9108      0.0     offender\n",
       "9109      0.0    offenders\n",
       "9110      0.0      offense\n",
       "9111      0.0     offenses\n",
       "9112      0.0        offer\n",
       "9113      0.0      offered\n",
       "9114      0.0       office\n",
       "9115      0.0      officer\n",
       "9116      0.0     officers\n",
       "9117      0.0      offices\n",
       "9118      0.0     official\n",
       "9119      0.0    officials\n",
       "9120      0.0       offset\n",
       "9121      0.0      offsets\n",
       "9122      0.0     offshore\n",
       "9123      0.0        often\n",
       "9124      0.0   oftentimes\n",
       "9125      0.0        ogden\n",
       "9126      0.0           oh\n",
       "9127      0.0         ohio\n",
       "9128      0.0      ohioans\n",
       "9129      0.0        ohkay\n",
       "9106      0.0       oetken\n",
       "9104      0.0      october\n",
       "9078      0.0  observation\n",
       "9103      0.0          ocs\n",
       "9080      0.0  observatory\n",
       "...       ...          ...\n",
       "4587      0.0         easy\n",
       "4588      0.0       eating\n",
       "4589      0.0        eaton\n",
       "4566      0.0         earn\n",
       "4565      0.0     earmarks\n",
       "4564      0.0   earmarking\n",
       "4551      0.0          e15\n",
       "4541      0.0         dyas\n",
       "4542      0.0    dyersburg\n",
       "4543      0.0        dyess\n",
       "4544      0.0        dying\n",
       "4545      0.0        dylan\n",
       "4546      0.0      dynamic\n",
       "4547      0.0     dynamics\n",
       "4548      0.0     dyslexia\n",
       "4549      0.0     dystonia\n",
       "4550      0.0    dystrophy\n",
       "4552      0.0          e85\n",
       "4563      0.0    earmarked\n",
       "4553      0.0          eab\n",
       "4554      0.0          eac\n",
       "4555      0.0         each\n",
       "4556      0.0        eager\n",
       "4557      0.0        eagle\n",
       "4558      0.0       eagles\n",
       "4559      0.0         earl\n",
       "4560      0.0      earlier\n",
       "4561      0.0        early\n",
       "4562      0.0      earmark\n",
       "13669     0.0         zuni\n",
       "\n",
       "[13670 rows x 2 columns]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "lala = \"It isnâ€™t enough to merely downsize government, having a smaller version of the same failed systems. We must do things in a dramatically different way by reversing the undermining of federalism and the centralizing of power in Washington. We look to the example set by Republican Governors and legislators all across the nation. Their leadership in reforming and reengineering government closest to the people vindicates the role of the States as the laboratories of democracy.Our approach, like theirs, is two-fold. We look to government â€“ local, State, and federal â€“ for the things government must do, but we believe those duties can be carried out more efficiently and at less cost. For all other activities, we look to the private sector; for the American peopleâ€™s resourcefulness, productivity, innovation, fiscal responsibility, and citizen-leadership have always been the true foundation of our national greatness For much of the last century, an opposing view has dominated public policy where we have witnessed the expansion, centralization, and bureaucracy in an entitlement society. Government has lumbered on, stifling innovation, with no incentive for fundamental change, through antiquated programs begun generations ago and now ill-suited to present needs and future requirements. As a result, todayâ€™s taxpayers â€“ and future generations â€“ face massive indebtedness, while Congressional Democrats and the current Administration block every attempt to turn things around. This man-made log-jam â€“ the so-called stalemate in Washington â€“ particularly affects the governmentâ€™s three largest programs, which have become central to the lives of untold millions of Americans: Medicare, Medicaid, and Social Security committed to saving Medicare and Medicaid. Unless the programsâ€™ fiscal ship is righted, the individuals hurt the first and the worst will be those who depend on them the most. We will save Medicare by modernizing it, by empowering its participants, and by putting it on a secure financial footing. This will be an enormous undertaking, and it should be a non-partisan one. We welcome to the effort all who sincerely want to ensure the future for our seniors and the poor. Republicans are determined to achieve that goal with a candid and honest presentation of Despite the enormous differences between Medicare and Medicaid, the two programs share the same fiscal outlook: their current courses cannot be sustained. Medicare has grown from more than 20 million enrolled in 1970 to more than 47 million enrolled today, with a projected total of 80 million in 2030. Medicaid counted almost 30 million enrollees in 1990, has about 54 million now, and under Obamacare would include an additional 11 million. Medicare spent more than $520 billion in 2010 and has close to $37 trillion in unfunded obligations, while total Medicaid spending will more than double by 2019. In many States, Medicaidâ€™s mandates and inflexible bureaucracy have become a budgetary black hole, growing faster than most other budget lines and devouring funding for many other essential governmental functions the problem and its solutions to the American people We are the party of government reform. At a time when the federal government has become bloated, antiquated and unresponsive to taxpayers, it is our intention not only to improve management and provide better services, but also to rethink and restructure government to bring it into the twenty-first century. Government reform requires constant vigilance and effort because government by its nature tends to expand in both size and scope. Our goal is not just less spending in Washington but something far more important for the future of our nation: protecting the constitutional rights of citizens, The problem goes beyond finances. Poor quality healthcare is the most expensive type of care because it prolongs affliction and leads to ever more complications. Even expensive prevention is preferable to more costly treatment later on. When approximately 80 percent of healthcare costs are related to lifestyle -smoking, obesity, substance abuse-far greater emphasis has to be put upon personal responsibility for health maintenance. Our goal for both Medicare and Medicaid must be to assure that every participant receives the amount of care they need at the time they need it, whether for an expectant mother and her baby or for someone in the last moments of life. The proper purpose of regulation is to set forth clear rules of the road for the citizens, so that business owners and workers can understand in advance what they need to do, or not do, to augment the possibilities for success within the confines of the law. Regulations must be drafted and implemented to balance legitimate public safety or consumer protection goals and job creation. Constructive regulation should be a helpful guide, not a punitive threat. Worst of all, over-regulation is a stealth tax on everyone as the costs of compliance with the whims of federal agencies are passed along to the consumers at the cost of $1.75 trillion a year. Many regulations are necessary, like those which ensure the safety of food and medicine, especially from overseas. But no peril justifies the regulatory impact of Obamacare on the practice of medicine, the Dodd-Frank Act on financial services, or the EPAâ€™s and OSHAâ€™s overreaching regulation agenda. A Republican Congress and President will repeal the first and second, and rein in the third. We support a sunset requirement to force reconsideration of out-of-date regulations, and we endorse pending legislation to require congressional approval for all new major and costly regulations Absent reforms, these two programs are headed for bankruptcy that will endanger care for seniors and the poor  sustainable prosperity, and strengthening the American family  I trust Iowans, Granite staters (ph), people in South Carolina, people in Nevada, to start this process out. I kind of miss Donald Trump. He was a little teddy bear to me We always had such a loving relationship in these debates and in between and the tweets. I kind of miss him. I wish he was here. Everybody else was in the witness protection program when I went after him on behalf of what the Republican cause should be: conservative principles, believing in limited government, believing in accountability. Leading by fixing the things that are broken. Look, I am in the establishment because my dad, the greatest man alive was president of the United States and my brother, who I adore as well as fantastic brother was president. Fine, I'll take it. I guess I'm part of the establishment Barbara Bush my mom I'll take that, too But this election is not about our pedigree, this is an election about people that are really hurting. We need a leader that will fix things and have a proven record to do it. And we need someone who will take on Hillary Clinton in November. Someone who has a proven record, who has been tested, who is totally transparent. I released 34 years of tax returns...and 300,000 e-mails in my government record. To get the information from Hillary Clinton, you need to get a subpoena from the FBI. Senator Christie, you began this campaign touting your record as a Republican from a blue state who knows how to get things done and reach across the aisle. However, many Republicans feel that reaching across the aisle and getting things done isn't great if you get the wrong things done. And they prefer to stand on principle rather than compromise. Why are they wrong and you're right? They're not wrong. But what's wrong is your premise in the question. You can do both. There is no reason why you can't stand for principles, go and fight for them and be able also, to have to get things done in government.You know, what people are frustrated about in Washington, D.C.., and I know the folks out there tonight are incredibly frustrated because what they see is a government that doesn't work for them. You know, for the 45-year-old construction worker out there, who is having a hard time making things meet.He's lost $4,000 in the last seven years in his income because of this administration. He doesn't want to hear the talk about politics Megyn and who is establishment and who is grassroots. And who's compromised and who is principled. What he wants is something to get done.And that's the difference between being a governor and having done that for the last six years in New Jersey and being someone who has never had to be responsible for any of those decisions. Barack Obama was never responsible for those decisions.Hillary Clinton has never been responsible for those kind of decisions where they were held accountable. I've been held accountable for six years as the governor of New Jersey and with a Democratic legislature, I've gotten conservative things done. That's exactly what I'll do as president of the United States.Senator Paul, you are definitely not in the establishment category But at the beginning of this campaign, you said you were your own man when asked about your father, former Texas Congressman and three-time presidential candidate Ron Paul\"\n",
    "features = vectorizer.get_feature_names()\n",
    "vectorized = pd.DataFrame(vectorizer.transform(['lala']).toarray(),columns=features)\n",
    "vectorized.head()\n",
    "print vectorizer.transform(['lala']).toarray()\n",
    "features\n",
    "new_words = pd.DataFrame({'words':features, 'counts':vectorizer.transform(['lala']).toarray()[0]}).sort_values(by='counts',ascending=False)\n",
    "new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13643\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "13643\n",
      "0.0\n",
      "class [1]\n",
      "probability [[ 0.42417667  0.57582333]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer.transform(['Something completely unrelated']).toarray()\n",
    "#vectorizer.vocabulary_.get('document')\n",
    "vectorizer.transform(['Something completely unrelated']).toarray()\n",
    "def reporter(list): \n",
    "    values = []\n",
    "    not_av = []\n",
    "    for word in list:\n",
    "        try:\n",
    "            val = vectorized[word][0]\n",
    "            values.append(val)\n",
    "        except KeyError:\n",
    "            values.append(0.0)\n",
    "    return values\n",
    "        \n",
    "values = reporter(word_column_names_capitol)\n",
    "print len(values)\n",
    "print values\n",
    "\n",
    "#X has 1 features per sample; expecting 13643\n",
    "print len(lr.coef_[0])\n",
    "val = vectorized[\"iraq\"][0]\n",
    "print val\n",
    "print \"class\", lr.predict(values)\n",
    "print \"probability\", lr.predict_proba(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0xe2 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-437-2f8ea8fc8436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#vectorized = pd.DataFrame(analyze(new_test),columns=features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 238\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0xe2 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "new_test = \"It isnâ€™t enough to merely downsize government, having a smaller version of the same failed systems. We must do things in a dramatically different way by reversing the undermining of federalism and the centralizing of power in Washington. We look to the example set by Republican Governors and legislators all across the nation. Their leadership in reforming and reengineering government closest to the people vindicates the role of the States as the laboratories of democracy.Our approach, like theirs, is two-fold. We look to government â€“ local, State, and federal â€“ for the things government must do, but we believe those duties can be carried out more efficiently and at less cost. For all other activities, we look to the private sector; for the American peopleâ€™s resourcefulness, productivity, innovation, fiscal responsibility, and citizen-leadership have always been the true foundation of our national greatness For much of the last century, an opposing view has dominated public policy where we have witnessed the expansion, centralization, and bureaucracy in an entitlement society. Government has lumbered on, stifling innovation, with no incentive for fundamental change, through antiquated programs begun generations ago and now ill-suited to present needs and future requirements. As a result, todayâ€™s taxpayers â€“ and future generations â€“ face massive indebtedness, while Congressional Democrats and the current Administration block every attempt to turn things around. This man-made log-jam â€“ the so-called stalemate in Washington â€“ particularly affects the governmentâ€™s three largest programs, which have become central to the lives of untold millions of Americans: Medicare, Medicaid, and Social Security committed to saving Medicare and Medicaid. Unless the programsâ€™ fiscal ship is righted, the individuals hurt the first and the worst will be those who depend on them the most. We will save Medicare by modernizing it, by empowering its participants, and by putting it on a secure financial footing. This will be an enormous undertaking, and it should be a non-partisan one. We welcome to the effort all who sincerely want to ensure the future for our seniors and the poor. Republicans are determined to achieve that goal with a candid and honest presentation of Despite the enormous differences between Medicare and Medicaid, the two programs share the same fiscal outlook: their current courses cannot be sustained. Medicare has grown from more than 20 million enrolled in 1970 to more than 47 million enrolled today, with a projected total of 80 million in 2030. Medicaid counted almost 30 million enrollees in 1990, has about 54 million now, and under Obamacare would include an additional 11 million. Medicare spent more than $520 billion in 2010 and has close to $37 trillion in unfunded obligations, while total Medicaid spending will more than double by 2019. In many States, Medicaidâ€™s mandates and inflexible bureaucracy have become a budgetary black hole, growing faster than most other budget lines and devouring funding for many other essential governmental functions the problem and its solutions to the American people We are the party of government reform. At a time when the federal government has become bloated, antiquated and unresponsive to taxpayers, it is our intention not only to improve management and provide better services, but also to rethink and restructure government to bring it into the twenty-first century. Government reform requires constant vigilance and effort because government by its nature tends to expand in both size and scope. Our goal is not just less spending in Washington but something far more important for the future of our nation: protecting the constitutional rights of citizens, The problem goes beyond finances. Poor quality healthcare is the most expensive type of care because it prolongs affliction and leads to ever more complications. Even expensive prevention is preferable to more costly treatment later on. When approximately 80 percent of healthcare costs are related to lifestyle -smoking, obesity, substance abuse-far greater emphasis has to be put upon personal responsibility for health maintenance. Our goal for both Medicare and Medicaid must be to assure that every participant receives the amount of care they need at the time they need it, whether for an expectant mother and her baby or for someone in the last moments of life. The proper purpose of regulation is to set forth clear rules of the road for the citizens, so that business owners and workers can understand in advance what they need to do, or not do, to augment the possibilities for success within the confines of the law. Regulations must be drafted and implemented to balance legitimate public safety or consumer protection goals and job creation. Constructive regulation should be a helpful guide, not a punitive threat. Worst of all, over-regulation is a stealth tax on everyone as the costs of compliance with the whims of federal agencies are passed along to the consumers at the cost of $1.75 trillion a year. Many regulations are necessary, like those which ensure the safety of food and medicine, especially from overseas. But no peril justifies the regulatory impact of Obamacare on the practice of medicine, the Dodd-Frank Act on financial services, or the EPAâ€™s and OSHAâ€™s overreaching regulation agenda. A Republican Congress and President will repeal the first and second, and rein in the third. We support a sunset requirement to force reconsideration of out-of-date regulations, and we endorse pending legislation to require congressional approval for all new major and costly regulations Absent reforms, these two programs are headed for bankruptcy that will endanger care for seniors and the poor  sustainable prosperity, and strengthening the American family  I trust Iowans, Granite staters (ph), people in South Carolina, people in Nevada, to start this process out. I kind of miss Donald Trump. He was a little teddy bear to me We always had such a loving relationship in these debates and in between and the tweets. I kind of miss him. I wish he was here. Everybody else was in the witness protection program when I went after him on behalf of what the Republican cause should be: conservative principles, believing in limited government, believing in accountability. Leading by fixing the things that are broken. Look, I am in the establishment because my dad, the greatest man alive was president of the United States and my brother, who I adore as well as fantastic brother was president. Fine, I'll take it. I guess I'm part of the establishment Barbara Bush my mom I'll take that, too But this election is not about our pedigree, this is an election about people that are really hurting. We need a leader that will fix things and have a proven record to do it. And we need someone who will take on Hillary Clinton in November. Someone who has a proven record, who has been tested, who is totally transparent. I released 34 years of tax returns...and 300,000 e-mails in my government record. To get the information from Hillary Clinton, you need to get a subpoena from the FBI. Senator Christie, you began this campaign touting your record as a Republican from a blue state who knows how to get things done and reach across the aisle. However, many Republicans feel that reaching across the aisle and getting things done isn't great if you get the wrong things done. And they prefer to stand on principle rather than compromise. Why are they wrong and you're right? They're not wrong. But what's wrong is your premise in the question. You can do both. There is no reason why you can't stand for principles, go and fight for them and be able also, to have to get things done in government.You know, what people are frustrated about in Washington, D.C.., and I know the folks out there tonight are incredibly frustrated because what they see is a government that doesn't work for them. You know, for the 45-year-old construction worker out there, who is having a hard time making things meet.He's lost $4,000 in the last seven years in his income because of this administration. He doesn't want to hear the talk about politics Megyn and who is establishment and who is grassroots. And who's compromised and who is principled. What he wants is something to get done.And that's the difference between being a governor and having done that for the last six years in New Jersey and being someone who has never had to be responsible for any of those decisions. Barack Obama was never responsible for those decisions.Hillary Clinton has never been responsible for those kind of decisions where they were held accountable. I've been held accountable for six years as the governor of New Jersey and with a Democratic legislature, I've gotten conservative things done. That's exactly what I'll do as president of the United States.Senator Paul, you are definitely not in the establishment category But at the beginning of this campaign, you said you were your own man when asked about your father, former Texas Congressman and three-time presidential candidate Ron Paul\"\n",
    "features = vectorizer.get_feature_names()\n",
    "#vectorized = pd.DataFrame(analyze(new_test),columns=features)\n",
    "vectorizer.transform(new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
